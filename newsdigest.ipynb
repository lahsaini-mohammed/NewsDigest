{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cuda avilabality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "from duckduckgo_search import DDGS\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_parse_xml(rss_xml_url:str='https://trends.google.com/trending/rss?geo=', region:str='US'):\n",
    "    \"\"\"\n",
    "    Fetches XML data (Get request) from a given URL and parses it into a dictionary (using xmltodict).\n",
    "    Args:\n",
    "        rss_xml_url (str): The URL of the RSS XML feed to fetch and parse. Defaults to realtime rss feed.\n",
    "        region (str): The region code to append to the URL. Defaults to 'US'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representation of the parsed XML data.\n",
    "              Returns None if an error occurs.\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If an error occurs while fetching the XML data.\n",
    "        xmltodict.expat.ExpatError: If an error occurs while parsing the XML data.\n",
    "        Exception: if any other unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch the XML data\n",
    "        response = requests.get(rss_xml_url+region)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        xml_data = response.content\n",
    "        # Parse the XML data and convert it to a dictionary\n",
    "        data_dict = xmltodict.parse(xml_data)\n",
    "        return data_dict\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the XML data: {e}\")\n",
    "    except xmltodict.expat.ExpatError as e:\n",
    "        print(f\"Error parsing the XML data: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE will choose USA as our geographic location because we will be using small LLMs and embedding models, however if you decide to go with flagship models (ex: GPT4-o...) you can go with non english speaking regions (refer to country_code file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the XML data\n",
    "#not all countries and region have a daily trending searches rrs feed (for example US, GB, FR... have while MA, DZ, TN don't)\n",
    "rss_xml_url_daily = 'https://trends.google.com/trends/trendingsearches/daily/rss?geo='\n",
    "rss_xml_url_realtime = 'https://trends.google.com/trending/rss?geo='\n",
    "\n",
    "# Fetch, parse, and print the XML data\n",
    "trends_dict = fetch_and_parse_xml()\n",
    "\n",
    "pprint(trends_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytrends is an unofficial, Python-based open source API for obtaining Google Trends data, but itâ€™s not the a reliable way to gather data and it's slow\n",
    "\n",
    "SerpApi offers several APIs to access Google data, including Google Trends but it's very expensive (startting from 75$ for 5000 searches/mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>entityNames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cristiano Ronaldo, Manchester United F.C., Eri...</td>\n",
       "      <td>[Cristiano Ronaldo, Manchester United F.C., Er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Federico Chiesa, Liverpool F.C., Arne Slot, Ju...</td>\n",
       "      <td>[Federico Chiesa, Liverpool F.C., Arne Slot, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manchester City F.C., Erling Haaland, Brentfor...</td>\n",
       "      <td>[Manchester City F.C., Erling Haaland, Brentfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane's Addiction, Dave Navarro, Perry Farrell,...</td>\n",
       "      <td>[Jane's Addiction, Dave Navarro, Perry Farrell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Southampton F.C., Premier League, Russell Mart...</td>\n",
       "      <td>[Southampton F.C., Premier League, Russell Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Villarreal CF, RCD Mallorca, LaLiga</td>\n",
       "      <td>[Villarreal CF, RCD Mallorca, LaLiga]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>West Ham United F.C., Fulham F.C., Premier League</td>\n",
       "      <td>[West Ham United F.C., Fulham F.C., Premier Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Matthijs de Ligt, Manchester United F.C., Neth...</td>\n",
       "      <td>[Matthijs de Ligt, Manchester United F.C., Net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Crystal Palace F.C., Leicester City F.C., Prem...</td>\n",
       "      <td>[Crystal Palace F.C., Leicester City F.C., Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Brighton &amp; Hove Albion F.C., Ipswich Town F.C....</td>\n",
       "      <td>[Brighton &amp; Hove Albion F.C., Ipswich Town F.C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mario Balotelli, Kerala Blasters FC, Mancheste...</td>\n",
       "      <td>[Mario Balotelli, Kerala Blasters FC, Manchest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FC Barcelona, Aston Villa F.C., Amadou Onana, ...</td>\n",
       "      <td>[FC Barcelona, Aston Villa F.C., Amadou Onana,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>South Carolina Gamecocks football, LSU Tigers ...</td>\n",
       "      <td>[South Carolina Gamecocks football, LSU Tigers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Azerbaijan Grand Prix, Lando Norris, McLaren, ...</td>\n",
       "      <td>[Azerbaijan Grand Prix, Lando Norris, McLaren,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bayer 04 Leverkusen, TSG 1899 Hoffenheim, Bund...</td>\n",
       "      <td>[Bayer 04 Leverkusen, TSG 1899 Hoffenheim, Bun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Celtic F.C., Heart of Midlothian F.C., Scottis...</td>\n",
       "      <td>[Celtic F.C., Heart of Midlothian F.C., Scotti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Burnley F.C., Leeds United, EFL Championship, ...</td>\n",
       "      <td>[Burnley F.C., Leeds United, EFL Championship,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>West Virginia Mountaineers football, Pittsburg...</td>\n",
       "      <td>[West Virginia Mountaineers football, Pittsbur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Tennis, Illinois Fighting Illini men's tennis,...</td>\n",
       "      <td>[Tennis, Illinois Fighting Illini men's tennis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tottenham Hotspur F.C., Noah Okafor, AC Milan,...</td>\n",
       "      <td>[Tottenham Hotspur F.C., Noah Okafor, AC Milan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>VfL Wolfsburg, Eintracht Frankfurt, Bundesliga</td>\n",
       "      <td>[VfL Wolfsburg, Eintracht Frankfurt, Bundesliga]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Alabama Crimson Tide football, Kalen DeBoer, A...</td>\n",
       "      <td>[Alabama Crimson Tide football, Kalen DeBoer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RB Leipzig, 1. FC Union Berlin, Bundesliga</td>\n",
       "      <td>[RB Leipzig, 1. FC Union Berlin, Bundesliga]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Cristiano Ronaldo, Manchester United F.C., Eri...   \n",
       "1   Federico Chiesa, Liverpool F.C., Arne Slot, Ju...   \n",
       "2   Manchester City F.C., Erling Haaland, Brentfor...   \n",
       "3   Jane's Addiction, Dave Navarro, Perry Farrell,...   \n",
       "4   Southampton F.C., Premier League, Russell Mart...   \n",
       "5                 Villarreal CF, RCD Mallorca, LaLiga   \n",
       "6   West Ham United F.C., Fulham F.C., Premier League   \n",
       "7   Matthijs de Ligt, Manchester United F.C., Neth...   \n",
       "8   Crystal Palace F.C., Leicester City F.C., Prem...   \n",
       "9   Brighton & Hove Albion F.C., Ipswich Town F.C....   \n",
       "10  Mario Balotelli, Kerala Blasters FC, Mancheste...   \n",
       "11  FC Barcelona, Aston Villa F.C., Amadou Onana, ...   \n",
       "12  South Carolina Gamecocks football, LSU Tigers ...   \n",
       "13  Azerbaijan Grand Prix, Lando Norris, McLaren, ...   \n",
       "14  Bayer 04 Leverkusen, TSG 1899 Hoffenheim, Bund...   \n",
       "15  Celtic F.C., Heart of Midlothian F.C., Scottis...   \n",
       "16  Burnley F.C., Leeds United, EFL Championship, ...   \n",
       "17  West Virginia Mountaineers football, Pittsburg...   \n",
       "18  Tennis, Illinois Fighting Illini men's tennis,...   \n",
       "19  Tottenham Hotspur F.C., Noah Okafor, AC Milan,...   \n",
       "20     VfL Wolfsburg, Eintracht Frankfurt, Bundesliga   \n",
       "21  Alabama Crimson Tide football, Kalen DeBoer, A...   \n",
       "22         RB Leipzig, 1. FC Union Berlin, Bundesliga   \n",
       "\n",
       "                                          entityNames  \n",
       "0   [Cristiano Ronaldo, Manchester United F.C., Er...  \n",
       "1   [Federico Chiesa, Liverpool F.C., Arne Slot, J...  \n",
       "2   [Manchester City F.C., Erling Haaland, Brentfo...  \n",
       "3   [Jane's Addiction, Dave Navarro, Perry Farrell...  \n",
       "4   [Southampton F.C., Premier League, Russell Mar...  \n",
       "5               [Villarreal CF, RCD Mallorca, LaLiga]  \n",
       "6   [West Ham United F.C., Fulham F.C., Premier Le...  \n",
       "7   [Matthijs de Ligt, Manchester United F.C., Net...  \n",
       "8   [Crystal Palace F.C., Leicester City F.C., Pre...  \n",
       "9   [Brighton & Hove Albion F.C., Ipswich Town F.C...  \n",
       "10  [Mario Balotelli, Kerala Blasters FC, Manchest...  \n",
       "11  [FC Barcelona, Aston Villa F.C., Amadou Onana,...  \n",
       "12  [South Carolina Gamecocks football, LSU Tigers...  \n",
       "13  [Azerbaijan Grand Prix, Lando Norris, McLaren,...  \n",
       "14  [Bayer 04 Leverkusen, TSG 1899 Hoffenheim, Bun...  \n",
       "15  [Celtic F.C., Heart of Midlothian F.C., Scotti...  \n",
       "16  [Burnley F.C., Leeds United, EFL Championship,...  \n",
       "17  [West Virginia Mountaineers football, Pittsbur...  \n",
       "18  [Tennis, Illinois Fighting Illini men's tennis...  \n",
       "19  [Tottenham Hotspur F.C., Noah Okafor, AC Milan...  \n",
       "20   [VfL Wolfsburg, Eintracht Frankfurt, Bundesliga]  \n",
       "21  [Alabama Crimson Tide football, Kalen DeBoer, ...  \n",
       "22       [RB Leipzig, 1. FC Union Berlin, Bundesliga]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytrends.request import TrendReq\n",
    "\n",
    "# Only need to run this once, the rest of requests will use the same session.\n",
    "pytrend = TrendReq()\n",
    "\n",
    "pytrend.realtime_trending_searches(pn='US') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create google trends dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize the input text by removing special characters and non-ASCII characters.#+\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Replaces HTML single quote code with an actual single quote.\n",
    "    2. Removes non-English (non-ASCII) characters.\n",
    "    3. Removes special characters and punctuation, keeping only letters, numbers, and spaces.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"&#39;\", \"'\")\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_google_dataframes(trends_dict):\n",
    "    \"\"\"\n",
    "    Creates a pandas DataFrame from a dictionary containing Google Trends data.\n",
    "\n",
    "    Args:\n",
    "        trends_dict (dict): A dictionary containing the Google Trends data. It should have the following structure:\n",
    "            - 'rss' (dict): A dictionary containing the RSS feed data.\n",
    "                - 'channel' (dict): A dictionary containing the channel data.\n",
    "                    - 'item' (list): A list of dictionaries containing the individual trend data.\n",
    "                        - 'title' (str): The title of the trend.\n",
    "                        - 'ht:approx_traffic' (int): The approximate traffic for the trend.\n",
    "                        - 'pubDate' (str): The publication date of the trend.\n",
    "                        - 'ht:news_item' (list or dict): A list or dictionary containing the news items for the trend.\n",
    "\n",
    "        Note: The 'ht:news_item' field can be either a list or a dictionary. If it is a list, each item in the list is a dictionary with the following keys:\n",
    "            - 'ht:news_item_url' (str): The URL of the news item.\n",
    "            - 'ht:news_item_title' (str): The title of the news item.\n",
    "\n",
    "            If it is a dictionary, it has the following keys:\n",
    "            - 'ht:news_item_url' (str): The URL of the news item.\n",
    "            - 'ht:news_item_title' (str): The title of the news item.\n",
    "\n",
    "    Returns:\n",
    "        google_trends_df (pandas.DataFrame): A DataFrame containing the Google Trends data. It has the following columns:\n",
    "            - 'trend_kws' (list): The titles of the trends.\n",
    "            - 'traffic' (list): The approximate traffic for the trends.\n",
    "            - 'pubDate' (list): The publication dates of the trends.\n",
    "            - 'url' (list): The URLs of the news items for each trend.\n",
    "            - 'title' (list): The titles of the news items for each trend.\n",
    "            - Note: The 'url' and 'title' columns may contain multiple values for each trend if there are multiple news items.\n",
    "\n",
    "    \"\"\"\n",
    "    google_trends_dict = {\"trend_kws\":[], \"traffic\":[], \"pubDate\":[], \"url\":[], \"title\":[]}\n",
    "    for trend in trends_dict['rss']['channel']['item']:\n",
    "        google_trends_dict['trend_kws'].append(trend['title'])\n",
    "        google_trends_dict['traffic'].append(trend['ht:approx_traffic'])\n",
    "        google_trends_dict['pubDate'].append(trend['pubDate'])\n",
    "        if isinstance(trend['ht:news_item'], list):\n",
    "            google_trends_dict['url'].append([news_item['ht:news_item_url'] for news_item in trend['ht:news_item']])\n",
    "            google_trends_dict['title'].append([news_item['ht:news_item_title'] for news_item in trend['ht:news_item']])\n",
    "        else:\n",
    "            google_trends_dict['url'].append([trend['ht:news_item']['ht:news_item_url']])\n",
    "            google_trends_dict['title'].append([trend['ht:news_item']['ht:news_item_title']])\n",
    "\n",
    "    google_trends_df = pd.DataFrame(google_trends_dict)\n",
    "    # clean each title\n",
    "    google_trends_df[\"title\"] = google_trends_df[\"title\"].map(lambda links: [clean_text(link) for link in links])\n",
    "    return google_trends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_df = create_google_dataframes(trends_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls that contain domains from domains_skip_list\n",
    "domains_skip_list = [\"msn.com\", \"nytimes.com\", \"washingtonpost.com\", \"e360.yale.edu\", \"star-telegram.com\"]\n",
    "for i in range(google_trends_df.shape[0]):\n",
    "    for j, url in enumerate(google_trends_df.loc[i,\"url\"]):\n",
    "        if any(domain in url for domain in domains_skip_list):\n",
    "            google_trends_df.loc[i,\"url\"].pop(j)\n",
    "            google_trends_df.loc[i,\"title\"].pop(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trend_kws</th>\n",
       "      <th>traffic</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missouri football</td>\n",
       "      <td>2000+</td>\n",
       "      <td>Sat, 14 Sep 2024 06:50:00 -0700</td>\n",
       "      <td>[https://www.espn.com/espn/betting/story/_/id/...</td>\n",
       "      <td>[Boston College vs Missouri betting Tigers off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utsa</td>\n",
       "      <td>200+</td>\n",
       "      <td>Sat, 14 Sep 2024 06:50:00 -0700</td>\n",
       "      <td>[https://www.statesman.com/story/sports/colleg...</td>\n",
       "      <td>[What channel is Texas vs UTSA on Time TV sche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sicario</td>\n",
       "      <td>200+</td>\n",
       "      <td>Sat, 14 Sep 2024 06:50:00 -0700</td>\n",
       "      <td>[https://www.eluniversal.com.co/sucesos/2024/0...</td>\n",
       "      <td>[Darmile Peralta el mototaxista que sicarios m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oregon vs oregon state</td>\n",
       "      <td>200+</td>\n",
       "      <td>Sat, 14 Sep 2024 06:50:00 -0700</td>\n",
       "      <td>[https://www.rollingstone.com/product-recommen...</td>\n",
       "      <td>[Oregon vs Oregon State Livestream How to Watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lioness movie</td>\n",
       "      <td>200+</td>\n",
       "      <td>Sat, 14 Sep 2024 06:50:00 -0700</td>\n",
       "      <td>[https://deadline.com/2024/09/lioness-season-2...</td>\n",
       "      <td>[Lioness Adds Max Martini Kirk Acevedo  Patric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                trend_kws traffic                          pubDate  \\\n",
       "0       missouri football   2000+  Sat, 14 Sep 2024 06:50:00 -0700   \n",
       "1                    utsa    200+  Sat, 14 Sep 2024 06:50:00 -0700   \n",
       "2                 sicario    200+  Sat, 14 Sep 2024 06:50:00 -0700   \n",
       "3  oregon vs oregon state    200+  Sat, 14 Sep 2024 06:50:00 -0700   \n",
       "4           lioness movie    200+  Sat, 14 Sep 2024 06:50:00 -0700   \n",
       "\n",
       "                                                 url  \\\n",
       "0  [https://www.espn.com/espn/betting/story/_/id/...   \n",
       "1  [https://www.statesman.com/story/sports/colleg...   \n",
       "2  [https://www.eluniversal.com.co/sucesos/2024/0...   \n",
       "3  [https://www.rollingstone.com/product-recommen...   \n",
       "4  [https://deadline.com/2024/09/lioness-season-2...   \n",
       "\n",
       "                                               title  \n",
       "0  [Boston College vs Missouri betting Tigers off...  \n",
       "1  [What channel is Texas vs UTSA on Time TV sche...  \n",
       "2  [Darmile Peralta el mototaxista que sicarios m...  \n",
       "3  [Oregon vs Oregon State Livestream How to Watc...  \n",
       "4  [Lioness Adds Max Martini Kirk Acevedo  Patric...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_trends_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "related news articles from ddg web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddg_dataframe(google_trends_df):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame of DDG news results for each trend keyword in the given Google trends DataFrame.\n",
    "\n",
    "    Args:\n",
    "        google_trends_df (pandas.DataFrame): The Google trends DataFrame containing the trend keywords.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame of DDG news results, with each row representing a news result.\n",
    "    \"\"\"\n",
    "\n",
    "    trends_news = []\n",
    "    for trend_kw in google_trends_df.trend_kws.to_list():\n",
    "        # searches for news article with the given keyword, using worldwide region, moderate safe search, and a maximum of 5 results.\n",
    "        results = DDGS().news(keywords=trend_kw, max_results=5)\n",
    "        filtered_results = [res for res in results if not any(domain in res['url'] for domain in domains_skip_list)]\n",
    "        filtered_results = list(map(lambda d: {'trend_kws':trend_kw, **d}, filtered_results[:3]))\n",
    "        trends_news.extend(filtered_results)\n",
    "    \n",
    "    trends_ddg_news_df = pd.DataFrame(trends_news)\n",
    "    return trends_ddg_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_ddg_news_df = create_ddg_dataframe(google_trends_df)\n",
    "trends_ddg_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_google_trends_with_ddg_news(google_trends_df, trends_ddg_news_df):\n",
    "    \"\"\"\n",
    "    Update the Google Trends DataFrame with additional URLs and titles from DuckDuckGo news search results.\n",
    "\n",
    "    This function iterates through each trend in the Google Trends DataFrame and finds corresponding\n",
    "    news articles from the DuckDuckGo news search results. It then appends these additional URLs and\n",
    "    titles to the existing lists in the Google Trends DataFrame.\n",
    "\n",
    "    Args:\n",
    "        google_trends_df (pandas.DataFrame): DataFrame containing Google Trends data.\n",
    "            Expected to have columns 'trend_kws', 'url', and 'title'.\n",
    "        trends_ddg_news_df (pandas.DataFrame): DataFrame containing DuckDuckGo news search results.\n",
    "            Expected to have columns 'trend_kws', 'url', and 'title'.\n",
    "\n",
    "    Returns:\n",
    "        None. The function modifies the google_trends_df in-place.\n",
    "    \"\"\"\n",
    "    for i, trend in enumerate(google_trends_df.trend_kws):\n",
    "        url_list = trends_ddg_news_df[trends_ddg_news_df[\"trend_kws\"]==trend]['url'].to_list()\n",
    "        title_list = trends_ddg_news_df[trends_ddg_news_df[\"trend_kws\"]==trend]['title'].to_list()\n",
    "        google_trends_df.loc[i, \"url\"].extend(url_list)\n",
    "        google_trends_df.loc[i, \"title\"].extend(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_google_trends_with_ddg_news(google_trends_df, trends_ddg_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import List, Optional\n",
    "from langchain_core.documents import Document\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "async def _fetch(session: aiohttp.ClientSession, url: str, timeout: int = 3) -> Optional[str]:\n",
    "    try:\n",
    "        async with session.get(url, timeout=timeout) as response:\n",
    "            return await response.text()\n",
    "    except asyncio.TimeoutError:\n",
    "        logger.warning(f\"Timeout occurred while fetching {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def _fetch_all(urls: List[str], max_concurrent: int = 100, timeout: int = 3) -> List[Optional[str]]:\n",
    "    connector = aiohttp.TCPConnector(limit=max_concurrent)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        tasks = [_fetch(session, url, timeout) for url in urls]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "def _parse_content(html: Optional[str], url: str) -> Optional[Document]:\n",
    "    if html is None:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text(separator='\\n', strip=True)\n",
    "    metadata = {\n",
    "        \"source\": url,\n",
    "        \"title\": soup.title.string if soup.title else \"No title\",\n",
    "    }\n",
    "    return Document(page_content=text, metadata=metadata)\n",
    "\n",
    "\n",
    "async def _url_loader(url_list: List[str], max_concurrent: int = 100, timeout: int = 3) -> List[Document]:\n",
    "    html_contents = await _fetch_all(url_list, max_concurrent, timeout)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=(os.cpu_count() or 4)*5) as executor:\n",
    "        documents = list(executor.map(_parse_content, html_contents, url_list))\n",
    "    \n",
    "    return [doc for doc in documents if doc is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By design asyncio does not allow its event loop to be nested. This presents a practical problem - when in an environment where the event loop is already running it's impossible to run tasks and wait for the result. Trying to do so will give the error RuntimeError - This event loop is already running. The issue pops up in various environments, such as web servers, GUI applications and in Jupyter notebooks. This module patches asyncio to allow nested use of asyncio.run and loop.rununtilcomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import nest_asyncio\n",
    "# # nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# define embedding model\n",
    "hf_api_key = \"\"\n",
    "# Initialize embeddings based on type\n",
    "embedding_type = 'api'\n",
    "hf_model_name= \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "if embedding_type == 'local':\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=hf_model_name,\n",
    "        model_kwargs={'device': device}\n",
    "    )\n",
    "elif embedding_type == 'api':\n",
    "    embeddings_model = HuggingFaceInferenceAPIEmbeddings(\n",
    "        api_key=hf_api_key,\n",
    "        model_name=hf_model_name\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"embedding_type must be either 'local' or 'api'\")\n",
    "\n",
    "\n",
    "# define llm\n",
    "llm = ChatGroq(\n",
    "        api_key=\"\",  # Replace with your actual Groq API key\n",
    "        model_name=\"llama-3.1-8b-instant\"\n",
    "    )\n",
    "\n",
    "\n",
    "# define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a helpful assistant that specializes in article summarization.\n",
    "            Your task is to summarize a given text article and generate a title for it.\n",
    "            If the provided article doesn't contain coherent and meaningful content, just return an empty response.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"Article: {article}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# define output schema for llm\n",
    "class SummmaryWithTitle(BaseModel):\n",
    "    '''Article summary and title.'''\n",
    "    title: str\n",
    "    summary: str\n",
    "\n",
    "dict_schema = convert_to_openai_tool(SummmaryWithTitle)\n",
    "structured_output_llm = llm.with_structured_output(dict_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rec_splitter(url_doc_list: List[Document]) -> List[Document]:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=50,\n",
    "            add_start_index=True\n",
    "        )\n",
    "        return text_splitter.split_documents(url_doc_list)\n",
    "\n",
    "def _retrieved_docs_parser(ret_doc_list: List[Document]) -> str:\n",
    "    ret_article = \"\\n\".join([doc.page_content for doc in ret_doc_list])\n",
    "    ret_article = re.sub(r'\\n+', '. ', ret_article)\n",
    "    sentence_pattern = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z][a-z][A-Z]\\.)(?<=\\.|\\?|!|\\n)\\s*')\n",
    "    ret_article_sentences = sentence_pattern.split(ret_article)\n",
    "    ret_article_meaningful_sentences = [s for s in ret_article_sentences if len(s.split()) > 5]\n",
    "    return '\\n'.join(ret_article_meaningful_sentences)\n",
    "retrieved_docs_parser_runnable = RunnableLambda(_retrieved_docs_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Any\n",
    "faiss_cache: Dict[str, FAISS] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_url_docs = []\n",
    "all_url_list = []\n",
    "trend_kws = google_trends_df.trend_kws.to_list()\n",
    "for i, trend_kw in enumerate(trend_kws):    \n",
    "    print(f\"Trend: {trend_kw}  {i+1}/{len(trend_kws)}\")\n",
    "    url_list = google_trends_df[google_trends_df[\"trend_kws\"]==trend_kw]['url'].iloc[0]\n",
    "    if url_list:\n",
    "        url_docs = await _url_loader(url_list)\n",
    "        for doc in url_docs:\n",
    "            if (not doc.page_content) and (doc.metadata[\"source\"] in trends_ddg_news_df.url.to_list()):\n",
    "                article_body_index = trends_ddg_news_df['url'].to_list().index(doc.metadata[\"source\"])\n",
    "                doc.page_content += trends_ddg_news_df['body'][article_body_index]\n",
    "        all_url_docs.append(url_docs)\n",
    "        all_url_list.append(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(model_name=hf_model_name,model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from typing import Dict\n",
    "faiss_cache: Dict[str, FAISS] = {}\n",
    "for i in range(len(all_url_docs)):\n",
    "    cache_key = frozenset(all_url_list[i])\n",
    "    if cache_key not in faiss_cache:\n",
    "        splits_docs = _rec_splitter(all_url_docs[i])\n",
    "        faiss_cache[cache_key]  = FAISS.from_documents(splits_docs, embeddings_model)\n",
    "    faiss_db = faiss_cache[cache_key]\n",
    "    faiss_retriever = faiss_db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(model_name=hf_model_name,model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from typing import Dict\n",
    "faiss_cache: Dict[str, FAISS] = {}\n",
    "for i in range(len(all_url_docs)):\n",
    "    cache_key = frozenset(all_url_list[i])\n",
    "    if cache_key not in faiss_cache:\n",
    "        splits_docs = _rec_splitter(all_url_docs[i])\n",
    "        faiss_cache[cache_key]  = FAISS.from_documents(splits_docs, embeddings_model)\n",
    "    faiss_db = faiss_cache[cache_key]\n",
    "    faiss_retriever = faiss_db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=hf_api_key,model_name=hf_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from typing import Dict\n",
    "faiss_cache: Dict[str, FAISS] = {}\n",
    "for i in range(len(all_url_docs)):\n",
    "    cache_key = frozenset(all_url_list[i])\n",
    "    if cache_key not in faiss_cache:\n",
    "        splits_docs = _rec_splitter(all_url_docs[i])\n",
    "        faiss_cache[cache_key]  = FAISS.from_documents(splits_docs, embeddings_model)\n",
    "    faiss_db = faiss_cache[cache_key]\n",
    "    faiss_retriever = faiss_db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(model_name=hf_model_name,model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define llm\n",
    "llm = ChatGroq(\n",
    "        api_key=\"\",  # Replace with your actual Groq API key\n",
    "        # model_name=\"llama-3.1-8b-instant\"\n",
    "        model_name=\"llama-3.1-70b-versatile\"\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "# define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a helpful assistant that specializes in article summarization.\n",
    "            Your task is to summarize a given text article and generate a title for it.\n",
    "            If the provided article doesn't contain coherent and meaningful content, just return an empty response.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"Article: {article}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# define output schema for llm\n",
    "class SummmaryWithTitle(BaseModel):\n",
    "    '''Article summary and title.'''\n",
    "    title: str\n",
    "    summary: str\n",
    "\n",
    "dict_schema = convert_to_openai_tool(SummmaryWithTitle)\n",
    "structured_output_llm = llm.with_structured_output(dict_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:00,224 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: missouri football  1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:01,336 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:01,336 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: utsa  2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:02,353 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:02,356 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: sicario  3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:03,201 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:03,203 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: oregon vs oregon state  4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:04,122 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:04,122 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: lioness movie  5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:05,173 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:05,173 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: fortnite  6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:06,059 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:06,059 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: arkansas football schedule  7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:06,964 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:06,964 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: pat mcafee  8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:07,775 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:07,775 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: dylan stewart  9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:09,126 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-14 19:52:09,126 - INFO - Performing RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend: directv disney  10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:52:10,160 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "trend_kws = google_trends_df.trend_kws.to_list()\n",
    "results = {\"Trend_kws\":[], \"Title\":[], \"Summary\":[]}\n",
    "for i, trend_kw in enumerate(trend_kws):    \n",
    "    print(f\"Trend: {trend_kw}  {i+1}/{len(trend_kws)}\")\n",
    "    df_trend = google_trends_df[google_trends_df[\"trend_kws\"]==trend_kw]\n",
    "    url_list = df_trend['url'].iloc[0]\n",
    "    if url_list:\n",
    "        url_docs = await _url_loader(url_list)\n",
    "        for doc in url_docs:\n",
    "            if (not doc.page_content) and (doc.metadata[\"source\"] in trends_ddg_news_df.url.to_list()):\n",
    "                article_body_index = trends_ddg_news_df['url'].to_list().index(doc.metadata[\"source\"])\n",
    "                doc.page_content += trends_ddg_news_df['body'][article_body_index]\n",
    "        print(\"## Creating FAISS vectorstore\")\n",
    "        cache_key = frozenset(url_list)\n",
    "        if cache_key not in faiss_cache:\n",
    "            splits_docs = _rec_splitter(url_docs)\n",
    "            faiss_cache[cache_key]  = FAISS.from_documents(splits_docs, embeddings_model)\n",
    "        faiss_db = faiss_cache[cache_key]\n",
    "        faiss_retriever = faiss_db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 10})\n",
    "        ret_query = '\\n'.join(df_trend['title'].iloc[0])\n",
    "\n",
    "        logger.info(\"Performing RAG\")\n",
    "        rag_chain = (\n",
    "            faiss_retriever\n",
    "            | {\"article\": retrieved_docs_parser_runnable}\n",
    "            | prompt_template\n",
    "            | structured_output_llm\n",
    "        )\n",
    "        rag_results = rag_chain.invoke(ret_query)\n",
    "        results['Trend_kws'].append(trend_kw)\n",
    "        results['Title'].append(rag_results['title'])\n",
    "        results['Summary'].append(rag_results['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trend_kws': ['missouri football',\n",
       "  'utsa',\n",
       "  'sicario',\n",
       "  'oregon vs oregon state',\n",
       "  'lioness movie',\n",
       "  'fortnite',\n",
       "  'arkansas football schedule',\n",
       "  'pat mcafee',\n",
       "  'dylan stewart',\n",
       "  'directv disney'],\n",
       " 'Title': ['Boston College vs. Missouri Tigers Prediction',\n",
       "  'UTSA Roadrunners vs. Texas Longhorns: Preview and Streaming Info',\n",
       "  \"Will Smith Exits Big-Budget Action Movie 'Sugar Bandits'\",\n",
       "  'Oregon vs Oregon State Football Game: How to Watch Online Without Cable',\n",
       "  'Lioness Adds New Cast Members for Season 2',\n",
       "  'How to Complete The Illusionist Quests in Fortnite Chapter 5, Season 4',\n",
       "  'Arkansas vs UAB: TV Channel, Time, and Streaming Info',\n",
       "  \"Pat McAfee Praises Shane Beamer, South Carolina Football Ahead of 'College GameDay'\",\n",
       "  'South Carolina Freshman Dylan Stewart Poised for Stardom',\n",
       "  'DirecTV and Disney Resolve Dispute, Restore Service for College Football'],\n",
       " 'Summary': ['The No. 6 Missouri Tigers are favored by 14.5 points against the No. 24 Boston College Eagles in their Week 3 college football matchup. Missouri possesses a higher level of talent on both sides of the ball and superior defense, but Boston College has skilled runners who can extend drives and eat up game clock.',\n",
       "  'The UTSA Roadrunners visit the No. 3 Texas Longhorns at DKR-Texas Memorial Stadium on Saturday. Texas is looking to avoid a letdown after passing its first test of the season with a win over Michigan. The game will be broadcast on ESPN at 6 p.m. CDT, and streaming options are available through Fubo and other services.',\n",
       "  \"Will Smith has stepped down from his role in the $80M action movie 'Sugar Bandits.' Despite this, Smith and his production company will remain producers on the project, which was set to begin filming in the coming months but had stalled due to uncertainty over Smith's on-screen commitment.\",\n",
       "  'The Oregon Ducks play the Oregon State Beavers today at 3:30 p.m. on FOX. Oddsmakers have the Oregon Ducks winning todayâ€™s matchup against the Oregon State Beavers, with an advantage of -1050 over the Beaversâ€™ +660. The game can be streamed online without cable through various services.',\n",
       "  'Max Martini, Kirk Acevedo, and Patricia De Leon will join the Season 2 cast of Taylor Sheridanâ€™s drama series Lioness. The show stars Zoe SaldaÃ±a, Laysla De Oliveira, Genesis Rodriguez, and Michael Kelly, with Morgan Freeman. Martini will play Tracer, a man hunter with the Special Forces team, while Acevedo will portray Gutierrez, a toughened FBI agent, and De Leon will play Maria, a wealthy Dallas resident.',\n",
       "  'In Fortnite Chapter 5, Season 4, players must complete The Illusionist Quests, which involve taking down Mysterio in Doomstadt and then using spray paint on various structures, including the central statue, to earn EXP rewards.',\n",
       "  'The Arkansas vs UAB college football game will air on the SEC Network, with Dave Neal and Aaron Murray calling the game from the booth. The game starts at 3:15 p.m. from Reynolds Razorback Stadium in Fayetteville. Streaming options include ESPNâ€™s subscription service and other platforms. The odds for the game are available on USA TODAY Sports Betting Scores Odds Hub.',\n",
       "  \"Pat McAfee praises South Carolina football and head coach Shane Beamer ahead of their appearance on ESPN's 'College GameDay'. McAfee went viral for picking the Gamecocks over Kentucky in Week 2.\",\n",
       "  \"Dylan Stewart, an 18-year-old true freshman defensive end at South Carolina, has been one of the Gamecocks' best players this season. He has taken the college football world by storm, drawing high praise from analysts and comparisons to future Hall of Famers. As the Gamecocks prepare to face LSU, Stewart is expected to continue his impressive performance.\",\n",
       "  'DirecTV and Disney have reached a long-term deal, ending a two-week blackout of Disney networks. The agreement comes just in time for college football, with service restored on Saturday. The dispute left over 11 million DirecTV subscribers without access to Disney channels, including ESPN, ABC, and the Disney Channel. The new deal settles the spat and returns the Disney-owned channels to DirecTV, DirecTV Stream, and U-Verse.']}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
